{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "tazxOW_uAuWH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load dataset\n",
        "df = pd.read_csv(\"/content/fake_and_real_news.csv\")\n",
        "\n",
        "print(df.head())\n",
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud6DhoqyAvOm",
        "outputId": "d1a242cf-bcb7-4e4d-8c20-53230a878e54"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Text label\n",
            "0   Top Trump Surrogate BRUTALLY Stabs Him In The...  Fake\n",
            "1  U.S. conservative leader optimistic of common ...  Real\n",
            "2  Trump proposes U.S. tax overhaul, stirs concer...  Real\n",
            "3   Court Forces Ohio To Allow Millions Of Illega...  Fake\n",
            "4  Democrats say Trump agrees to work on immigrat...  Real\n",
            "label\n",
            "Fake    5000\n",
            "Real    4900\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Clean text\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # remove links\n",
        "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # keep only letters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df['Text'] = df['Text'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "BGdk5EPyAzVW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n",
        "# Real -> 1, Fake -> 0"
      ],
      "metadata": {
        "id": "PY43piqpA2En"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['label'],\n",
        "                                                    test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "Qto2i5QuA637"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Tokenize\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "1pU8c09KA8zT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "GxN7K4eTBDSv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 200"
      ],
      "metadata": {
        "id": "OZjWRt3nBGKV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n"
      ],
      "metadata": {
        "id": "yAzTj7WgBGeF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=128, input_length=max_len),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dropout(0.3),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')  # binary classification\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtvQObYMBIbP",
        "outputId": "11f7b184-73cc-4162-a223-dd5e0cdca3cf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "i6HqsXSTBKJ1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Train\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_test_pad, y_test),\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07wiu0wiBMe4",
        "outputId": "e4331afd-94f9-4cc1-ef1d-63373987a92d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 538ms/step - accuracy: 0.8986 - loss: 0.2238 - val_accuracy: 0.9995 - val_loss: 0.0032\n",
            "Epoch 2/5\n",
            "\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 486ms/step - accuracy: 0.9991 - loss: 0.0027 - val_accuracy: 0.9990 - val_loss: 0.0048\n",
            "Epoch 3/5\n",
            "\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 502ms/step - accuracy: 1.0000 - loss: 4.1209e-04 - val_accuracy: 0.9995 - val_loss: 0.0028\n",
            "Epoch 4/5\n",
            "\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 511ms/step - accuracy: 1.0000 - loss: 8.9901e-05 - val_accuracy: 0.9995 - val_loss: 0.0030\n",
            "Epoch 5/5\n",
            "\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 505ms/step - accuracy: 1.0000 - loss: 4.6068e-05 - val_accuracy: 0.9995 - val_loss: 0.0033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Evaluate\n",
        "loss, acc = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"✅ Test Accuracy: {acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T5NDg-ABOG6",
        "outputId": "7e299e70-dbc6-4320-dd0e-4be06d4cdda1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Test Accuracy: 99.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Save model\n",
        "model.save(\"fake_news_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69jzpfw3BQk4",
        "outputId": "e73dbcde-c656-4080-a921-d66658552705"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save tokenizer\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)"
      ],
      "metadata": {
        "id": "lK6xaEDUD50Q"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pickle\n",
        "\n",
        "# 1. Load dataset\n",
        "df = pd.read_csv(\"/content/fake_and_real_news.csv\")   # replace with your file name\n",
        "df.columns = [\"text\", \"label\"]  # make sure columns are correct\n",
        "\n",
        "# 2. Encode labels (Fake = 0, Real = 1)\n",
        "df['label'] = df['label'].map({'Fake': 0, 'Real': 1})\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'], df['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# 5. Save vectorizer as pickle\n",
        "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "print(\"✅ vectorizer.pkl file created successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5NugggfEATg",
        "outputId": "8339dc99-b7c2-4796-ce65-92f813e03346"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ vectorizer.pkl file created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Wcv4GdNEob2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}